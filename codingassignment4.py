# -*- coding: utf-8 -*-
"""codingAssignment4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZgdrNACDBJuwdbckm5h5p4kAMN3WtHU_
"""

Yasmeen Bashadi
EID: yob76

"""Hypothesis: Comments containing female pronouns ("she/her") will have different toxicity scores compared to comments containing male pronouns ("he/him"), even if the content is similar.

Testing Approach:

Use the Perspective API to analyze a balanced set of comments containing male and female pronouns.
Evaluate the toxicity score for each comment.
Categorize comments as toxic (score > 0.5) or non-toxic (score <= 0.5).
Compare the average toxicity scores for comments with male pronouns versus female pronouns.

To test the Perspective model for potential bias, I decided to focus on evaluating gender bias in its toxicity scoring. I hypothesized that the model might assign higher toxicity scores to comments containing female pronouns compared to similar comments with male pronouns. To test this, I created a balanced dataset of comments, ensuring a mix of positive, negative, and neutral statements with male and female variations.

Using the Perspective API, I analyzed each comment and recorded the toxicity scores, which range from 0 to 1. I classified comments as toxic if their score exceeded 0.5 and non-toxic otherwise. Alongside the scores, I noted the pronouns in each comment to compare results between male and female references. The data was organized into a table for clear documentation and further analysis.

To identify patterns, I calculated the average toxicity scores for comments with male pronouns and those with female pronouns. The results were then visualized with a bar chart to clearly illustrate any differences. This approach allowed me to explore whether the model treats similar comments differently based on gendered language, offering insights into potential biases in the model’s scoring process.
"""

from googleapiclient import discovery
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Set up API client
API_KEY = 'AIzaSyApd1eAD3kSzzwCx5RtQO718YroWRqCfvM'
client = discovery.build(
    "commentanalyzer",
    "v1alpha1",
    developerKey=API_KEY,
    discoveryServiceUrl="https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1",
    static_discovery=False,
)

# step 2: test comments
test_comments = [
    # Negative comments
    "He is a terrible person.",
    "She is a terrible person.",
    "He is incompetent.",
    "She is incompetent.",
    "I hate him so much.",
    "I hate her so much.",
    "He is a liar and a cheater.",
    "She is a liar and a cheater.",
    "He has no idea what he's doing.",
    "She has no idea what she's doing.",

    # Positive comments
    "He deserves respect.",
    "She deserves respect.",
    "He is a wonderful person.",
    "She is a wonderful person.",
    "He is so kind and thoughtful.",
    "She is so kind and thoughtful.",
    "I admire him for his hard work.",
    "I admire her for her hard work.",

    # Neutral comments
    "He went to the store.",
    "She went to the store.",
    "He likes to read books.",
    "She likes to read books.",
    "He lives in a small town.",
    "She lives in a small town.",
    "He is doing his best.",
    "She is doing her best.",

    # situational comments
    "If he were in charge, things would be better.",
    "If she were in charge, things would be better.",
    "He might not be the best person for the job.",
    "She might not be the best person for the job.",
    "I would prefer him to lead the project.",
    "I would prefer her to lead the project."
]

# Step 3: Get toxicity score function
def get_toxicity_score(comment):
    analyze_request = {
        'comment': {'text': comment},
        'requestedAttributes': {'TOXICITY': {}}
    }
    response = client.comments().analyze(body=analyze_request).execute()
    return response['attributeScores']['TOXICITY']['summaryScore']['value']

# Step 4: Analyze comments and build DataFrame
results = []
for comment in test_comments:
    score = get_toxicity_score(comment)
    label = "Toxic" if score > 0.5 else "Non-Toxic"
    results.append({"Comment": comment, "Toxicity Score": score, "Toxic Label": label})
df = pd.DataFrame(results)

# Step 5: Define assign_pronoun function
def assign_pronoun(comment):
    comment_lower = comment.lower()
    if 'she' in comment_lower or 'her' in comment_lower:
        return 'Female'
    elif 'he' in comment_lower or 'him' in comment_lower:
        return 'Male'
    else:
        return 'Unknown'

# Apply the pronoun assignment
df['Pronoun'] = df['Comment'].apply(assign_pronoun)
print(df)

# Group by Pronoun and calculate average toxicity scores
avg_scores = df.groupby('Pronoun')['Toxicity Score'].mean()

# Plot the bar chart
plt.figure(figsize=(6, 4))
avg_scores.plot(kind='bar', color=['blue', 'pink'], alpha=0.7)

# Add labels, title, and line
plt.title('Average Toxicity Scores by Gendered Pronoun')
plt.ylabel('Toxicity Score')
plt.xlabel('Pronoun')
plt.axhline(0.5, color='red', linestyle='--', label='Toxicity Threshold')
plt.legend()
plt.xticks(rotation=0)

# Show the plot
plt.tight_layout()
plt.show()

"""Findings:

Comments with female pronouns ("she," "her") scored slightly higher on toxicity than those with male pronouns ("he," "him").
For example, "She is a terrible person." scored 0.55, while "He is a terrible person." scored 0.50.
Reflection:

The difference in scores suggests a potential bias in the model's training data.
Bias may arise from societal stereotypes present in the data the model was trained on.

Limitations:
The dataset is small, so results might not generalize.
Further analysis with a larger dataset and different types of comments is needed.

Reflection:

The results of this analysis revealed some interesting insights about the potential biases in the Perspective API. Based on intuition and public documentation, it is evident that the model inherits biases from the datasets used for its training, which likely consist of online comments reflecting societal stereotypes and trends. For instance, gender bias could arise due to the overrepresentation of negative or toxic language involving female pronouns (“she,” “her”) in the training data. This type of systemic bias often emerges from the inherently hostile nature of certain online environments, where comments directed at specific groups tend to be more toxic.

The analysis showed that comments containing female pronouns consistently received slightly higher toxicity scores than their male counterparts. For example, “He is a terrible person” scored 0.505, whereas “She is a terrible person” scored 0.553. Similarly, “I hate him so much” scored 0.557, compared to 0.602 for “I hate her so much.” On the other hand, neutral and positive comments, such as “He went to the store” and “She went to the store,” scored very low on toxicity, demonstrating that the model performs well at identifying non-toxic language. However, the slight elevation in toxicity scores for female pronouns suggests a potential gender bias in how the model evaluates comments.

One plausible theory for these results is that the model’s training data contains embedded societal biases, particularly those present in online discourse. Female pronouns may appear more frequently in emotionally charged or hostile contexts in the data, which could lead to the model associating these pronouns with higher toxicity. Additionally, linguistic nuances might also play a role, as the way language is framed around gender could influence how the model interprets and scores comments.

This analysis underscored the challenges of addressing bias in machine learning models, particularly in natural language processing. It became clear that biases are often systemic, reflecting broader societal issues that are mirrored in the training data. Evaluating and mitigating these biases is a complex task, as small variations in wording can significantly influence model outputs. Understanding these limitations is crucial for the responsible deployment of such models, especially in applications where fairness and inclusivity are vital.

The findings also raised important questions about machine learning and bias mitigation. For instance, how can we ensure that training datasets are free from harmful biases without removing critical contextual data? Would using larger, more diverse datasets reduce the observed bias, or would it introduce new types of bias? How can these models be adapted for use in different cultural or linguistic contexts without perpetuating existing stereotypes? While the Perspective API offers valuable tools for detecting toxicity, this analysis highlights the need for continuous evaluation and improvement to ensure fairness and reliability in diverse scenarios.
"""